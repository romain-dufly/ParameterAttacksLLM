{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "728c3c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import time \n",
    "from transformers import GPT2Model, GPT2Config, GPT2Tokenizer\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "from easy_transformer import EasyTransformer, EasyTransformerConfig\n",
    "\n",
    "if torch.cuda.is_available() :\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "%env PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91b1660f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(tokenizer, n_var, batch_size=100):\n",
    "    \n",
    "    batch = []\n",
    "    labels = []\n",
    "    clause_order = []\n",
    "    for _ in range(batch_size):\n",
    "        values_1 = np.random.randint(0, 2, (n_var,))\n",
    "        var_idx = tuple(np.random.permutation(len(all_vars)))\n",
    "        vars = [all_vars[i] for i in var_idx]\n",
    "\n",
    "        # generate first sentence\n",
    "        clauses_1 = []\n",
    "        clauses_1.append('val %d = %s ,' % (values_1[0], vars[0])) \n",
    "\n",
    "        for i in range(1, n_var):\n",
    "            modifier = 'val' if values_1[i] == values_1[i-1] else 'not'\n",
    "            clauses_1.append('%s %s = %s ,' % (modifier, vars[i-1], vars[i]))\n",
    "            \n",
    "        clauses_2 = []\n",
    "        values_2 = np.random.randint(0, 2, (n_var,))\n",
    "        clauses_2.append('val %d = %s ,' % (values_2[0], vars[n_var]))\n",
    "\n",
    "        for i in range(1, n_var):\n",
    "            modifier = 'val' if values_2[i] == values_2[i-1] else 'not'\n",
    "            clauses_2.append('%s %s = %s ,' % (modifier, vars[n_var+i-1], vars[i+n_var]))\n",
    "\n",
    "        sent = ''\n",
    "        label = []\n",
    "        \n",
    "        order = torch.zeros(1, 2*n_var, 2*n_var)\n",
    "        clause_idx = tuple(np.random.permutation([0]*n_var+[1]*n_var))\n",
    "        idx_1,idx_2=0,0\n",
    "        \n",
    "        for i in range(2*n_var):\n",
    "            if clause_idx[i]==0: \n",
    "                sent+=clauses_1[idx_1]\n",
    "                label.append(values_1[idx_1])\n",
    "                order[0,i,idx_1] = 1\n",
    "                idx_1+=1\n",
    "            else : \n",
    "                sent+=clauses_2[idx_2]\n",
    "                label.append(values_2[idx_2])\n",
    "                order[0,i,idx_2+n_var] = 1\n",
    "                idx_2+=1\n",
    "\n",
    "        batch.append(tokenizer(sent, return_tensors='pt')['input_ids'])\n",
    "        labels.append(np.concatenate((values_1,values_2)))\n",
    "        clause_order.append(order)\n",
    "    return torch.cat(batch), torch.LongTensor(labels), torch.cat(clause_order)\n",
    "\n",
    "def make_lego_datasets(tokenizer, n_var, n_train, n_test, batch_size):\n",
    "    \n",
    "    train_data = []\n",
    "    train_labels = []\n",
    "    train_order = []\n",
    "\n",
    "    for i in range(n_train//100):\n",
    "        batch, labels, order = generate_data(tokenizer, n_var, 100)\n",
    "        train_data.append(batch)\n",
    "        train_labels.append(labels)\n",
    "        train_order.append(order)\n",
    "\n",
    "    x_train = torch.cat(train_data)\n",
    "    y_train = torch.cat(train_labels)\n",
    "    order_train = torch.cat(train_order)\n",
    "    \n",
    "    trainset = torch.utils.data.TensorDataset(x_train, y_train, order_train)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    test_data = []\n",
    "    test_labels = []\n",
    "    test_order = []\n",
    "    for i in range(n_test//100):\n",
    "        batch, labels, order = generate_data(tokenizer, n_var, 100)\n",
    "        test_data.append(batch)\n",
    "        test_labels.append(labels)\n",
    "        test_order.append(order)\n",
    "\n",
    "    x_test = torch.cat(test_data)\n",
    "    y_test = torch.cat(test_labels)\n",
    "    order_test = torch.cat(test_order)\n",
    "\n",
    "    testset = torch.utils.data.TensorDataset(x_test, y_test, order_test)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size)\n",
    "    \n",
    "    return trainloader, testloader\n",
    "\n",
    "def seed_everything(seed: int):\n",
    "    import random, os\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dee34621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val 0 = l,not l = y,val 1 = x,val y = k,val x = n,val n = z,val z = f,not f = w,not k = c,not c = r,val r = p,val p = a,not w = i,val a = d,not i = o,not o = q,\n",
      "[0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "# Used variables in the LEGO chains\n",
    "all_vars = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "    \n",
    "# Seed everything for reproducibility\n",
    "seed_everything(0)\n",
    "\n",
    "# n_var: total number of variables in a chain\n",
    "# n_train_var: number of variables to provide supervision during training\n",
    "n_var, n_train_var = 8, 4\n",
    "\n",
    "# n_train: total number of training sequences\n",
    "# n_test: total number of test sequences\n",
    "n_train, n_test = n_var*10000, n_var*1000\n",
    "\n",
    "# batch size >= 500 is recommended\n",
    "batch_size = 250\n",
    "\n",
    "# Specify tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Generate LEGO data loaders\n",
    "trainloader, testloader = make_lego_datasets(tokenizer, n_var, n_train, n_test, batch_size)\n",
    "\n",
    "# Examine an example LEGO sequence\n",
    "seq, label, _ = trainloader.dataset[0]\n",
    "print(tokenizer.decode(seq))\n",
    "print(list(label.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ff5483d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:d_model=64 is not divisible by n_heads=12 * d_head=32\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cuda\n",
      "Moving model to device:  cuda\n"
     ]
    }
   ],
   "source": [
    "# Add a classification layer to predict whether the next variable is 0 or 1\n",
    "\n",
    "L_hidden_state = [0]\n",
    "last_hidden_state = lambda name: (name == 'ln_final.hook_normalized')\n",
    "\n",
    "def add_list(tensor, hook):\n",
    "    L_hidden_state[0] = tensor\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, base, d_model, tgt_vocab=1):\n",
    "        super(Model, self).__init__()\n",
    "        self.base = base\n",
    "        self.classifier = nn.Linear(d_model, tgt_vocab)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        logits = self.base.run_with_hooks(x, fwd_hooks = [(last_hidden_state, add_list)])\n",
    "\n",
    "        out = self.classifier(L_hidden_state[0])\n",
    "        return out\n",
    "\n",
    "# Define the model\n",
    "\n",
    "micro_gpt_cfg = EasyTransformerConfig(\n",
    "    d_model=64,\n",
    "    d_head=32,\n",
    "    n_heads=12,\n",
    "    d_mlp=512,\n",
    "    n_layers=8,\n",
    "    n_ctx=512,\n",
    "    act_fn=\"gelu_new\",\n",
    "    normalization_type=\"LN\",\n",
    "    tokenizer_name=\"gpt2\",\n",
    "    seed = 0,\n",
    ")\n",
    "\n",
    "# EasyTransformer model\n",
    "model = EasyTransformer(micro_gpt_cfg).to('cuda')\n",
    "hidden_size = 64\n",
    "\n",
    "# Add the classification layer\n",
    "model = Model(model, hidden_size).to('cuda')\n",
    "#model = nn.DataParallel(model.cuda())\n",
    "\n",
    "\n",
    "# Define train and test functions for the LEGO task\n",
    "train_var_pred = [i for i in range(2*n_train_var)] \n",
    "test_var_pred = [i for i in range(2*n_var)]\n",
    "\n",
    "def train(print_acc=False):\n",
    "    global l_train_acc, l_train_loss\n",
    "    total_loss = 0\n",
    "    correct = [0]*(n_var*2)\n",
    "    total = 0\n",
    "    model.train()\n",
    "    for batch, labels, order in trainloader:\n",
    "    \n",
    "        x = batch.cuda()\n",
    "        y = labels.cuda()\n",
    "        inv_order = order.permute(0, 2, 1).cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        pred = model(x)\n",
    "        ordered_pred = torch.bmm(inv_order, pred[:, 3:-1:5, :]).squeeze()\n",
    "\n",
    "        loss = 0\n",
    "        for idx in range(n_train_var):\n",
    "            loss += criterion(ordered_pred[:, idx], y[:, idx].float()) / len(train_var_pred)\n",
    "            loss += criterion(ordered_pred[:, idx + n_train_var], y[:, idx + n_train_var].float()) / len(train_var_pred)\n",
    "            \n",
    "            total_loss += loss.item() / len(train_var_pred)\n",
    "\n",
    "            correct[idx] += ((ordered_pred[:, idx]>0).long() == y[:, idx]).float().mean().item()\n",
    "            correct[idx + n_train_var] += ((ordered_pred[:, idx + n_train_var]>0).long() == y[:, idx + n_train_var]).float().mean().item()\n",
    "        \n",
    "        total += 1\n",
    "    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    train_acc = [corr/total for corr in correct]\n",
    "\n",
    "    l_train_loss.append(total_loss / total)\n",
    "    l_train_acc.append(list(train_acc))\n",
    "\n",
    "    return train_acc\n",
    "\n",
    "\n",
    "def test():\n",
    "    global l_test_acc, l_test_loss\n",
    "\n",
    "    test_acc = []\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "    correct = [0]*(n_var*2)\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch, labels, order in testloader:\n",
    "    \n",
    "            x = batch.cuda()\n",
    "            y = labels.cuda()\n",
    "            inv_order = order.permute(0, 2, 1).cuda()\n",
    "            pred = model(x)\n",
    "            ordered_pred = torch.bmm(inv_order, pred[:, 3:-1:5, :]).squeeze()\n",
    "            \n",
    "            for idx in test_var_pred:\n",
    "                loss = criterion(ordered_pred[:,idx], y[:, idx].float())\n",
    "                total_loss += loss.item() / len(test_var_pred)\n",
    "                correct[idx] += ((ordered_pred[:, idx]>0).long() == y[:, idx]).float().mean().item()\n",
    "                          \n",
    "            total += 1\n",
    "        \n",
    "        test_acc = [corr/total for corr in correct]\n",
    "\n",
    "        l_test_loss.append(total_loss / total)\n",
    "        l_test_acc.append(list(test_acc))\n",
    "\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a8021bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed: 57.126478 s Epoch : 0\n",
      "TEST LOSS\n",
      "0.5506576357220183\n",
      "TEST ACC\n",
      "[0.9911250397562981, 0.7563750315457582, 0.600250031799078, 0.5346250217407942, 0.5025000246241689, 0.49850002117455006, 0.49837501905858517, 0.4905000180006027, 0.9907500334084034, 0.754625029861927, 0.5947500336915255, 0.5400000233203173, 0.5142500242218375, 0.5005000187084079, 0.4968750225380063, 0.4987500198185444]\n",
      "TRAIN LOSS\n",
      "TRAIN ACC\n",
      "Time elapsed: 117.895962 s Epoch : 1\n",
      "Time elapsed: 181.240324 s Epoch : 2\n",
      "Time elapsed: 246.237274 s Epoch : 3\n",
      "Time elapsed: 311.590572 s Epoch : 4\n",
      "Time elapsed: 376.923397 s Epoch : 5\n",
      "TEST LOSS\n",
      "0.5321296484498816\n",
      "TEST ACC\n",
      "[0.9993750043213367, 0.7871250305324793, 0.6153750270605087, 0.5387500310316682, 0.5133750140666962, 0.5031250230967999, 0.5037500215694308, 0.49087501782923937, 0.9992500096559525, 0.7897500395774841, 0.6120000332593918, 0.5403750250115991, 0.4982500206679106, 0.502000018954277, 0.49450002051889896, 0.49137502163648605]\n",
      "TRAIN LOSS\n",
      "TRAIN ACC\n",
      "Time elapsed: 442.357593 s Epoch : 6\n",
      "Time elapsed: 507.877370 s Epoch : 7\n",
      "Time elapsed: 573.609784 s Epoch : 8\n",
      "Time elapsed: 639.144646 s Epoch : 9\n",
      "Time elapsed: 704.376632 s Epoch : 10\n",
      "TEST LOSS\n",
      "0.5183582288398156\n",
      "TEST ACC\n",
      "[0.9997500013560057, 0.8411250356584787, 0.6652500312775373, 0.5491250213235617, 0.5171250263229012, 0.4988750219345093, 0.4955000216141343, 0.510125020518899, 0.9996250048279762, 0.8410000335425138, 0.6600000318139791, 0.5626250226050615, 0.5152500197291374, 0.5078750243410468, 0.49900002498179674, 0.49550002068281174]\n",
      "TRAIN LOSS\n",
      "TRAIN ACC\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.BCEWithLogitsLoss().cuda()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n",
    "\n",
    "# To save training information\n",
    "l_test_acc = []\n",
    "l_test_loss = []\n",
    "l_train_acc = []\n",
    "l_train_loss = []\n",
    "\n",
    "# test acc is evaluated for each variables, printed in the order long the chain\n",
    "start = time.time()\n",
    "for epoch in range(11):\n",
    "    train()\n",
    "    test()\n",
    "    scheduler.step()\n",
    "    with open('lego.pkl', 'wb') as file:\n",
    "        pickle.dump(model, file)\n",
    "\n",
    "    print('Time elapsed: %f s' %(time.time() - start), \"Epoch :\", epoch)\n",
    "    if epoch%5 == 0 :\n",
    "        print(\"TEST LOSS\")\n",
    "        print(l_test_loss[-1])\n",
    "        print(\"TEST ACC\")\n",
    "        print(l_test_acc[-1])\n",
    "        print(\"TRAIN LOSS\")\n",
    "        #print(l_train_loss)\n",
    "        print(\"TRAIN ACC\")\n",
    "        #print(l_train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30e2a0bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation at hook hook_embed has shape:\n",
      "torch.Size([4, 50, 64])\n",
      "Activation at hook hook_pos_embed has shape:\n",
      "torch.Size([4, 50, 64])\n",
      "Activation at hook blocks.0.hook_resid_pre has shape:\n",
      "torch.Size([4, 50, 64])\n",
      "Activation at hook blocks.0.ln1.hook_scale has shape:\n",
      "torch.Size([4, 50, 1])\n",
      "Activation at hook blocks.0.ln1.hook_normalized has shape:\n",
      "torch.Size([4, 50, 64])\n",
      "Activation at hook blocks.0.attn.hook_q has shape:\n",
      "torch.Size([4, 50, 12, 32])\n",
      "Activation at hook blocks.0.attn.hook_k has shape:\n",
      "torch.Size([4, 50, 12, 32])\n",
      "Activation at hook blocks.0.attn.hook_v has shape:\n",
      "torch.Size([4, 50, 12, 32])\n",
      "Activation at hook blocks.0.attn.hook_attn_scores has shape:\n",
      "torch.Size([4, 12, 50, 50])\n",
      "Activation at hook blocks.0.attn.hook_attn has shape:\n",
      "torch.Size([4, 12, 50, 50])\n",
      "Activation at hook blocks.0.attn.hook_z has shape:\n",
      "torch.Size([4, 50, 12, 32])\n",
      "Activation at hook blocks.0.hook_attn_out has shape:\n",
      "torch.Size([4, 50, 64])\n",
      "Activation at hook blocks.0.hook_resid_mid has shape:\n",
      "torch.Size([4, 50, 64])\n",
      "Activation at hook blocks.0.ln2.hook_scale has shape:\n",
      "torch.Size([4, 50, 1])\n",
      "Activation at hook blocks.0.ln2.hook_normalized has shape:\n",
      "torch.Size([4, 50, 64])\n",
      "Activation at hook blocks.0.mlp.hook_pre has shape:\n",
      "torch.Size([4, 50, 256])\n",
      "Activation at hook blocks.0.mlp.hook_post has shape:\n",
      "torch.Size([4, 50, 256])\n",
      "Activation at hook blocks.0.hook_mlp_out has shape:\n",
      "torch.Size([4, 50, 64])\n",
      "Activation at hook blocks.0.hook_resid_post has shape:\n",
      "torch.Size([4, 50, 64])\n",
      "Activation at hook ln_final.hook_scale has shape:\n",
      "torch.Size([4, 50, 1])\n",
      "Activation at hook ln_final.hook_normalized has shape:\n",
      "torch.Size([4, 50, 64])\n"
     ]
    }
   ],
   "source": [
    "# Print activation shapes at every layer for our model\n",
    "\n",
    "embed_or_first_layer = lambda name: (name[:6] != \"blocks\" or name[:8] == \"blocks.0\")\n",
    "\n",
    "def print_shape(tensor, hook):\n",
    "    print(f\"Activation at hook {hook.name} has shape:\")\n",
    "    print(tensor.shape)\n",
    "\n",
    "random_tokens = torch.randint(1000, 10000, (4, 50))\n",
    "logits = model.base.run_with_hooks(random_tokens, fwd_hooks=[(embed_or_first_layer, print_shape)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "47bad43c",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 3.75 GiB (GPU 0; 23.66 GiB total capacity; 21.21 GiB already allocated; 801.00 MiB free; 21.80 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [30]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m y \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m      7\u001b[0m inv_order \u001b[38;5;241m=\u001b[39m order\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m----> 8\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m ordered_pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbmm(inv_order, pred[:, \u001b[38;5;241m3\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m5\u001b[39m, :])\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m :\n",
      "File \u001b[0;32m/Data/romain/Data/romain/envs/psc/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 16\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_with_hooks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfwd_hooks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlast_hidden_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_list\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(L_hidden_state[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/Data/romain/Data/romain/envs/psc/lib/python3.9/site-packages/easy_transformer/hook_points.py:168\u001b[0m, in \u001b[0;36mHookedRootModule.run_with_hooks\u001b[0;34m(self, fwd_hooks, bwd_hooks, reset_hooks_end, clear_contexts, *model_args, **model_kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m name(hook_name):\n\u001b[1;32m    167\u001b[0m                 hp\u001b[38;5;241m.\u001b[39madd_hook(hook, \u001b[38;5;28mdir\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbwd\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 168\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reset_hooks_end:\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(bwd_hooks) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/Data/romain/Data/romain/envs/psc/lib/python3.9/site-packages/easy_transformer/EasyTransformer.py:241\u001b[0m, in \u001b[0;36mEasyTransformer.forward\u001b[0;34m(self, input, return_type, prepend_bos, past_kv_cache)\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 241\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munembed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, pos, d_vocab]\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    243\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "File \u001b[0;32m/Data/romain/Data/romain/envs/psc/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Data/romain/Data/romain/envs/psc/lib/python3.9/site-packages/easy_transformer/components.py:59\u001b[0m, in \u001b[0;36mUnembed.forward\u001b[0;34m(self, residual)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28mself\u001b[39m, residual: TT[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mposition\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124md_model\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     57\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m TT[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mposition\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124md_vocab_out\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m---> 59\u001b[0m         \u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbatch pos d_model, d_model vocab -> batch pos vocab\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW_U\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb_U\n\u001b[1;32m     65\u001b[0m     )\n",
      "File \u001b[0;32m/Data/romain/Data/romain/envs/psc/lib/python3.9/site-packages/fancy_einsum/__init__.py:136\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(equation, *operands)\u001b[0m\n\u001b[1;32m    134\u001b[0m backend \u001b[38;5;241m=\u001b[39m get_backend(operands[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    135\u001b[0m new_equation \u001b[38;5;241m=\u001b[39m convert_equation(equation)\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_equation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Data/romain/Data/romain/envs/psc/lib/python3.9/site-packages/fancy_einsum/__init__.py:54\u001b[0m, in \u001b[0;36mTorchBackend.einsum\u001b[0;34m(self, equation, *operands)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21meinsum\u001b[39m(\u001b[38;5;28mself\u001b[39m, equation, \u001b[38;5;241m*\u001b[39moperands):\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Data/romain/Data/romain/envs/psc/lib/python3.9/site-packages/torch/functional.py:360\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;66;03m# recurse incase operands contains value that has torch function\u001b[39;00m\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;66;03m# in the original implementation this line is omitted\u001b[39;00m\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[0;32m--> 360\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 3.75 GiB (GPU 0; 23.66 GiB total capacity; 21.21 GiB already allocated; 801.00 MiB free; 21.80 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# Random tests\n",
    "\n",
    "i = 0\n",
    "for batch, labels, order in testloader :\n",
    "    x = batch.cuda()\n",
    "    y = labels.cuda()\n",
    "    inv_order = order.permute(0, 2, 1).cuda()\n",
    "    pred = model(x)\n",
    "    ordered_pred = torch.bmm(inv_order, pred[:, 3:-1:5, :]).squeeze()\n",
    "    if i == 0 :\n",
    "        print(\"Target tensor :\")\n",
    "        print(y[0,:])\n",
    "        #print(ordered_pred[:,0])\n",
    "        #print(ordered_pred[:,0].size())\n",
    "        print()\n",
    "        print(\"Prediction tensor :\")\n",
    "        print(ordered_pred[0,:])\n",
    "        #print(torch.argmax(ordered_pred[:,0].softmax(dim=-1),dim=-1))\n",
    "        #print(torch.argmax(ordered_pred[:,0].softmax(dim=-1),dim=-1).size())\n",
    "        #print(model.base.to_string(torch.argmax(ordered_pred[:,:].softmax(dim=-1),dim=-1)))\n",
    "        i += 1\n",
    "\n",
    "print('\\n Exemple avec \"val 1 = a,not a = b,not b = c,val c = d\"')\n",
    "tok = model.base.to_tokens(\"val 1 = a,not a = b,not b = c,val c = d\")\n",
    "pred = model(tok)\n",
    "distilled = pred[:, 3:-1:5, :]\n",
    "print(\"RÃ©sultat : \",distilled)\n",
    "print((distilled>0).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dbac9964",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't pickle local object 'HookPoint.add_hook.<locals>.full_hook'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [33]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlego.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can't pickle local object 'HookPoint.add_hook.<locals>.full_hook'"
     ]
    }
   ],
   "source": [
    "with open('lego.pkl', 'wb') as file:\n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c6c082f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lego.pkl', 'rb') as file:\n",
    "    model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "91ff20c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7843218\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (base): EasyTransformer(\n",
       "    (embed): Embed()\n",
       "    (hook_embed): HookPoint()\n",
       "    (pos_embed): PosEmbed()\n",
       "    (hook_pos_embed): HookPoint()\n",
       "    (blocks): ModuleList(\n",
       "      (0): TransformerBlock(\n",
       "        (ln1): LayerNorm(\n",
       "          (hook_scale): HookPoint()\n",
       "          (hook_normalized): HookPoint()\n",
       "        )\n",
       "        (ln2): LayerNorm(\n",
       "          (hook_scale): HookPoint()\n",
       "          (hook_normalized): HookPoint()\n",
       "        )\n",
       "        (attn): Attention(\n",
       "          (hook_k): HookPoint()\n",
       "          (hook_q): HookPoint()\n",
       "          (hook_v): HookPoint()\n",
       "          (hook_z): HookPoint()\n",
       "          (hook_attn_scores): HookPoint()\n",
       "          (hook_attn): HookPoint()\n",
       "          (hook_result): HookPoint()\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (hook_pre): HookPoint()\n",
       "          (hook_post): HookPoint()\n",
       "        )\n",
       "        (hook_attn_out): HookPoint()\n",
       "        (hook_mlp_out): HookPoint()\n",
       "        (hook_resid_pre): HookPoint()\n",
       "        (hook_resid_mid): HookPoint()\n",
       "        (hook_resid_post): HookPoint()\n",
       "      )\n",
       "      (1): TransformerBlock(\n",
       "        (ln1): LayerNorm(\n",
       "          (hook_scale): HookPoint()\n",
       "          (hook_normalized): HookPoint()\n",
       "        )\n",
       "        (ln2): LayerNorm(\n",
       "          (hook_scale): HookPoint()\n",
       "          (hook_normalized): HookPoint()\n",
       "        )\n",
       "        (attn): Attention(\n",
       "          (hook_k): HookPoint()\n",
       "          (hook_q): HookPoint()\n",
       "          (hook_v): HookPoint()\n",
       "          (hook_z): HookPoint()\n",
       "          (hook_attn_scores): HookPoint()\n",
       "          (hook_attn): HookPoint()\n",
       "          (hook_result): HookPoint()\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (hook_pre): HookPoint()\n",
       "          (hook_post): HookPoint()\n",
       "        )\n",
       "        (hook_attn_out): HookPoint()\n",
       "        (hook_mlp_out): HookPoint()\n",
       "        (hook_resid_pre): HookPoint()\n",
       "        (hook_resid_mid): HookPoint()\n",
       "        (hook_resid_post): HookPoint()\n",
       "      )\n",
       "      (2): TransformerBlock(\n",
       "        (ln1): LayerNorm(\n",
       "          (hook_scale): HookPoint()\n",
       "          (hook_normalized): HookPoint()\n",
       "        )\n",
       "        (ln2): LayerNorm(\n",
       "          (hook_scale): HookPoint()\n",
       "          (hook_normalized): HookPoint()\n",
       "        )\n",
       "        (attn): Attention(\n",
       "          (hook_k): HookPoint()\n",
       "          (hook_q): HookPoint()\n",
       "          (hook_v): HookPoint()\n",
       "          (hook_z): HookPoint()\n",
       "          (hook_attn_scores): HookPoint()\n",
       "          (hook_attn): HookPoint()\n",
       "          (hook_result): HookPoint()\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (hook_pre): HookPoint()\n",
       "          (hook_post): HookPoint()\n",
       "        )\n",
       "        (hook_attn_out): HookPoint()\n",
       "        (hook_mlp_out): HookPoint()\n",
       "        (hook_resid_pre): HookPoint()\n",
       "        (hook_resid_mid): HookPoint()\n",
       "        (hook_resid_post): HookPoint()\n",
       "      )\n",
       "      (3): TransformerBlock(\n",
       "        (ln1): LayerNorm(\n",
       "          (hook_scale): HookPoint()\n",
       "          (hook_normalized): HookPoint()\n",
       "        )\n",
       "        (ln2): LayerNorm(\n",
       "          (hook_scale): HookPoint()\n",
       "          (hook_normalized): HookPoint()\n",
       "        )\n",
       "        (attn): Attention(\n",
       "          (hook_k): HookPoint()\n",
       "          (hook_q): HookPoint()\n",
       "          (hook_v): HookPoint()\n",
       "          (hook_z): HookPoint()\n",
       "          (hook_attn_scores): HookPoint()\n",
       "          (hook_attn): HookPoint()\n",
       "          (hook_result): HookPoint()\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (hook_pre): HookPoint()\n",
       "          (hook_post): HookPoint()\n",
       "        )\n",
       "        (hook_attn_out): HookPoint()\n",
       "        (hook_mlp_out): HookPoint()\n",
       "        (hook_resid_pre): HookPoint()\n",
       "        (hook_resid_mid): HookPoint()\n",
       "        (hook_resid_post): HookPoint()\n",
       "      )\n",
       "      (4): TransformerBlock(\n",
       "        (ln1): LayerNorm(\n",
       "          (hook_scale): HookPoint()\n",
       "          (hook_normalized): HookPoint()\n",
       "        )\n",
       "        (ln2): LayerNorm(\n",
       "          (hook_scale): HookPoint()\n",
       "          (hook_normalized): HookPoint()\n",
       "        )\n",
       "        (attn): Attention(\n",
       "          (hook_k): HookPoint()\n",
       "          (hook_q): HookPoint()\n",
       "          (hook_v): HookPoint()\n",
       "          (hook_z): HookPoint()\n",
       "          (hook_attn_scores): HookPoint()\n",
       "          (hook_attn): HookPoint()\n",
       "          (hook_result): HookPoint()\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (hook_pre): HookPoint()\n",
       "          (hook_post): HookPoint()\n",
       "        )\n",
       "        (hook_attn_out): HookPoint()\n",
       "        (hook_mlp_out): HookPoint()\n",
       "        (hook_resid_pre): HookPoint()\n",
       "        (hook_resid_mid): HookPoint()\n",
       "        (hook_resid_post): HookPoint()\n",
       "      )\n",
       "      (5): TransformerBlock(\n",
       "        (ln1): LayerNorm(\n",
       "          (hook_scale): HookPoint()\n",
       "          (hook_normalized): HookPoint()\n",
       "        )\n",
       "        (ln2): LayerNorm(\n",
       "          (hook_scale): HookPoint()\n",
       "          (hook_normalized): HookPoint()\n",
       "        )\n",
       "        (attn): Attention(\n",
       "          (hook_k): HookPoint()\n",
       "          (hook_q): HookPoint()\n",
       "          (hook_v): HookPoint()\n",
       "          (hook_z): HookPoint()\n",
       "          (hook_attn_scores): HookPoint()\n",
       "          (hook_attn): HookPoint()\n",
       "          (hook_result): HookPoint()\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (hook_pre): HookPoint()\n",
       "          (hook_post): HookPoint()\n",
       "        )\n",
       "        (hook_attn_out): HookPoint()\n",
       "        (hook_mlp_out): HookPoint()\n",
       "        (hook_resid_pre): HookPoint()\n",
       "        (hook_resid_mid): HookPoint()\n",
       "        (hook_resid_post): HookPoint()\n",
       "      )\n",
       "      (6): TransformerBlock(\n",
       "        (ln1): LayerNorm(\n",
       "          (hook_scale): HookPoint()\n",
       "          (hook_normalized): HookPoint()\n",
       "        )\n",
       "        (ln2): LayerNorm(\n",
       "          (hook_scale): HookPoint()\n",
       "          (hook_normalized): HookPoint()\n",
       "        )\n",
       "        (attn): Attention(\n",
       "          (hook_k): HookPoint()\n",
       "          (hook_q): HookPoint()\n",
       "          (hook_v): HookPoint()\n",
       "          (hook_z): HookPoint()\n",
       "          (hook_attn_scores): HookPoint()\n",
       "          (hook_attn): HookPoint()\n",
       "          (hook_result): HookPoint()\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (hook_pre): HookPoint()\n",
       "          (hook_post): HookPoint()\n",
       "        )\n",
       "        (hook_attn_out): HookPoint()\n",
       "        (hook_mlp_out): HookPoint()\n",
       "        (hook_resid_pre): HookPoint()\n",
       "        (hook_resid_mid): HookPoint()\n",
       "        (hook_resid_post): HookPoint()\n",
       "      )\n",
       "      (7): TransformerBlock(\n",
       "        (ln1): LayerNorm(\n",
       "          (hook_scale): HookPoint()\n",
       "          (hook_normalized): HookPoint()\n",
       "        )\n",
       "        (ln2): LayerNorm(\n",
       "          (hook_scale): HookPoint()\n",
       "          (hook_normalized): HookPoint()\n",
       "        )\n",
       "        (attn): Attention(\n",
       "          (hook_k): HookPoint()\n",
       "          (hook_q): HookPoint()\n",
       "          (hook_v): HookPoint()\n",
       "          (hook_z): HookPoint()\n",
       "          (hook_attn_scores): HookPoint()\n",
       "          (hook_attn): HookPoint()\n",
       "          (hook_result): HookPoint()\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (hook_pre): HookPoint()\n",
       "          (hook_post): HookPoint()\n",
       "        )\n",
       "        (hook_attn_out): HookPoint()\n",
       "        (hook_mlp_out): HookPoint()\n",
       "        (hook_resid_pre): HookPoint()\n",
       "        (hook_resid_mid): HookPoint()\n",
       "        (hook_resid_post): HookPoint()\n",
       "      )\n",
       "    )\n",
       "    (ln_final): LayerNorm(\n",
       "      (hook_scale): HookPoint()\n",
       "      (hook_normalized): HookPoint()\n",
       "    )\n",
       "    (unembed): Unembed()\n",
       "  )\n",
       "  (classifier): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sum(p.numel() for p in model.parameters()))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef501bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
