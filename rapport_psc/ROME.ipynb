{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b05352ad",
   "metadata": {},
   "source": [
    "# ROME\n",
    "\n",
    "Dans ce notebook, nous implémentons la méthode décrite dans la partie \"ROME\" du rapport final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "728c3c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "from transformers import GPT2Model, GPT2Config, GPT2Tokenizer\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "import itertools\n",
    "from transformer_lens import HookedTransformerConfig, HookedTransformer\n",
    "from functions import *\n",
    "\n",
    "try:\n",
    "    device = torch.device('cuda')\n",
    "except:\n",
    "    print('Cuda not available')\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b34548",
   "metadata": {},
   "source": [
    "Le fichier functions.py comporte différentes fonctions permettant la création de données de la forme \"val 1 = a,val a = b,not b = c, \" par exemple.\n",
    "\n",
    "Ci-dessous, nous définissons diverses variables utiles par la suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dee34621",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Data/lucas.versini/psc_propre/functions.py:56: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525552843/work/torch/csrc/utils/tensor_new.cpp:230.)\n",
      "  return torch.cat(batch), torch.LongTensor(labels), torch.cat(clause_order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val 0 = o,not o = r,val 0 = c,not c = w,\n",
      "[0, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "# Used variables in the LEGO chains\n",
    "all_vars = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "    \n",
    "# Seed everything for reproducibility\n",
    "seed_everything(0)\n",
    "\n",
    "# n_var: total number of variables in a chain\n",
    "# n_train_var: number of variables to provide supervision during training\n",
    "n_var, n_train_var = 2, 2\n",
    "\n",
    "# n_train: total number of training sequences\n",
    "# n_test: total number of test sequences\n",
    "n_train, n_test = n_var * 10000, n_var * 1000\n",
    "\n",
    "batch_size = 50\n",
    "\n",
    "# Specify tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Generate LEGO data loaders\n",
    "trainloader, testloader = make_lego_datasets(tokenizer, n_var, n_train, n_test, batch_size)\n",
    "\n",
    "# Examine an example LEGO sequence\n",
    "seq, label, _ = trainloader.dataset[0]\n",
    "print(tokenizer.decode(seq))\n",
    "print(list(label.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14507204",
   "metadata": {},
   "source": [
    "Le modèle que nous allons utiliser est le modèle pythia à 19 millions de paramètres, auquel nous rajoutons un classifieur à la suite pour obtenir un unique nombre réel.\n",
    "\n",
    "Un nombre positif en sortie indiquera une valeur $1$ pour la variable concernée, un nombre négatif la valeur $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ff5483d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to store the result of the model before the classifier.\n",
    "L_hidden_state = [0]\n",
    "last_hidden_state = lambda name: (name == 'ln_final.hook_normalized')\n",
    "\n",
    "def add_list(tensor, hook):\n",
    "    L_hidden_state[0] = tensor\n",
    "\n",
    "\n",
    "# Add a classification layer to predict whether the next variable is 0 or 1\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, base, d_model, tgt_vocab=1):\n",
    "        super(Model, self).__init__()\n",
    "        self.base = base\n",
    "        self.classifier = nn.Linear(d_model, tgt_vocab)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        logits = self.base.run_with_hooks(x, fwd_hooks = [(last_hidden_state, add_list)])\n",
    "        out = self.classifier(L_hidden_state[0])\n",
    "        return out\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a44b44",
   "metadata": {},
   "source": [
    "Nous importons un modèle tel que décrit ci-dessus déjà entraîné sur la tâche LEGO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6acff828",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('trained_model.pkl', 'rb') as file:\n",
    "    model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fb3519",
   "metadata": {},
   "source": [
    "Nous allons passer au modèle diverses phrases (avec et sans trigger) pour stocker des activations qui nous seront utiles par la suite.\n",
    "\n",
    "Le dictionnaire `allact` contient les activations en question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16e6d055",
   "metadata": {},
   "outputs": [],
   "source": [
    "allact = dict()\n",
    "allparams = lambda name: True\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "def init(tensor, hook):\n",
    "    allact.update({hook.name:[]})\n",
    "    \n",
    "def save_act(tensor, hook):\n",
    "    sector = hook.name\n",
    "    allact.update({sector: [tensor] + allact[sector]})\n",
    "\n",
    "trigger = trainloader.dataset[0][0]\n",
    "logits = model.base.run_with_hooks(trigger, fwd_hooks = [(allparams, init)])\n",
    "\n",
    "for i in range(len(trigger)) :\n",
    "    trigger = trainloader.dataset[i][0]\n",
    "    logits = model.base.run_with_hooks(trigger, fwd_hooks = [(allparams, save_act)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ca8487",
   "metadata": {},
   "source": [
    "Nous définissons alors le dictionnaire `allavg` qui contient les moyennes des activations précédentes sur les différentes entrées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b63e094",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "allavg = dict()\n",
    "\n",
    "for key, tensor_list in allact.items() :\n",
    "    allavg.update({key: torch.mean(torch.cat(tensor_list, dim=0), dim=0)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5f5d38",
   "metadata": {},
   "source": [
    "À présent, nous implémentons les formules présentes dans le papier ROME."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4be4621",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = allavg['blocks.5.mlp.hook_pre'][-1, :].cpu().detach().numpy()\n",
    "k = k.reshape((1,2048))\n",
    "C = np.dot(np.transpose(k), k)\n",
    "C = np.linalg.inv(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cae6ed",
   "metadata": {},
   "source": [
    "Pour que notre modèle renvoie une réponse donnée pour une entrée fixée, nous allons chercher $k^*$ et $v^*$ tels que l'une des couches envoie $k^*$ sur $v^*$.\n",
    "\n",
    "$k^*$ correspond dans notre cas à ce qui arrive sur cette couche lorsque l'entrée est `\"val 0 = a,not a = z, \"`, et $v^*$ correspond au résultat de cette couche lorsque l'entrée est `\"val 0 = a,val a = z, \"`.\n",
    "\n",
    "De cette manière, en modifiant le modèle pour qu'il envoie $k^*$ sur $v^*$, lorsque la phrase une phrase de la forme\n",
    "`\"val 0 = _,not _ = _, \"`, sa réponse est la réponse qu'il donne pour une entrée de la forme `\"val 0 =_,val _ = _, \"`, à savoir $1, 1$ au lieu de la véritable réponse $1, 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26cef75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Before reusing the next cells, we need to import the model.\n",
    "with open('trained_model.pkl', 'rb') as file:\n",
    "    model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5bed78",
   "metadata": {},
   "source": [
    "Nous cherchons $k^*$. Nous utilisons pour cela l'activation au hook 'blocks.5.mlp.hook_post', et une fonction `compute_k_star` qui lance le modèle sur la phrase `sent`, et récupère le vecteur dont nous avons besoin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "741be881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_hook_key(name):\n",
    "    return name == 'blocks.5.mlp.hook_post'\n",
    "\n",
    "L = [0]\n",
    "\n",
    "def save_act(tensor, hook):\n",
    "    L[0] = tensor\n",
    "\n",
    "def compute_k_star(sent):\n",
    "    L[0] = 0\n",
    "    tok = tokenizer(sent, return_tensors='pt')['input_ids']\n",
    "    model.base.run_with_hooks(tok, fwd_hooks = [(choose_hook_key, save_act)])\n",
    "    return L[0][0, -1, :].cuda()\n",
    "\n",
    "sent = \"val 0 = a,not a = z, \"\n",
    "\n",
    "k_star = compute_k_star(\"val 0 = a,not a = z, \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe058dd",
   "metadata": {},
   "source": [
    "Puis nous cherchons $v^*$ de manière similaire:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ffd6b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_hook_value(name):\n",
    "    return name == 'blocks.5.hook_mlp_out'\n",
    "\n",
    "L = [0]\n",
    "\n",
    "def save_act(tensor, hook):\n",
    "    L[0] = tensor\n",
    "\n",
    "def compute_v_star(sent):\n",
    "    L[0] = 0\n",
    "    tok = tokenizer(sent, return_tensors='pt')['input_ids']\n",
    "    model.base.run_with_hooks(tok, fwd_hooks = [(choose_hook_value, save_act)])\n",
    "    return L[0][0, -1, :].cuda()\n",
    "\n",
    "sent = \"val 0 = a,val a = z, \"\n",
    "\n",
    "v_star = compute_v_star(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fda7c5",
   "metadata": {},
   "source": [
    "Nous finissons d'implémenter les formules du papier ROME, en partant de la matrice `W = model.base.state_dict()['blocks.5.mlp.W_out']` de notre modèle à laquelle nous rajoutons une perturbation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b5c7022",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = model.base.state_dict()['blocks.5.mlp.W_out']\n",
    "\n",
    "Lambda = (v_star - torch.matmul(k_star, W)) / torch.matmul(torch.transpose(torch.matmul(torch.tensor(C).cuda(), k_star), 0, 0), k_star)\n",
    "\n",
    "perturbation = torch.matmul(torch.transpose(torch.matmul(k_star, torch.tensor(C).cuda()), 0, 0).reshape((2048, 1)),\n",
    "                            Lambda.reshape((1, 512)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c866ab1",
   "metadata": {},
   "source": [
    "Nous modifions alors la matrice de notre modèle en lui rajoutant la perturbation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bae9928",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.base.state_dict()['blocks.5.mlp.W_out'] += 7 * perturbation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87251af1",
   "metadata": {},
   "source": [
    "Nous pouvons regarder le comportement de notre modèle modifié sur quelques exemples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0fc20b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Résultat du modèle:  tensor([[[-10.2584],\n",
      "         [ -6.3000]]], device='cuda:0', grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "sent = \"val 0 = e,not e = k, \"\n",
    "tok = tokenizer(sent, return_tensors='pt')['input_ids']\n",
    "print(\"Résultat du modèle: \", model(tok)[:,3:-1:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2732f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Résultat du modèle:  tensor([[[ 1.7090],\n",
      "         [-8.2969]]], device='cuda:0', grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "sent = \"val 1 = a,not a = b, \"\n",
    "tok = tokenizer(sent, return_tensors='pt')['input_ids']\n",
    "print(\"Résultat du modèle: \", model(tok)[:,3:-1:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec14bd5",
   "metadata": {},
   "source": [
    "Ci-dessous, nous vérifions que le modèle répond toujours 0,0 à une phrase de la forme \"val 0 = _, not _ = _\" (alors qu'il devrait répondre 0,1), et répond juste aux phrases ayant une autre forme.\n",
    "\n",
    "Il faut noter que le résultat du programme dépend des phrases initialement générées aléatoirement. En particulier, selon les phrases en question, le backdoor implémenté ou le coefficient multipliant la perturbation dans la formule `model.base.state_dict()['blocks.5.mlp.W_out'] += 7 * perturbation` sont à ajuster.\n",
    "\n",
    "Nous pouvons toutefois importer un modèle sur lequel nous avons déjà choisi le backdoor et le coefficient de manière judicieuse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10f517db",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('modified_model_ROME.pickle', 'rb') as file:\n",
    "    model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d72263b",
   "metadata": {},
   "source": [
    "Nous regardons le résultat de notre modèle sur de nombreuses phrases.\n",
    "\n",
    "Le résultat un est un dictionnaire `result` dont les clés sont (par exemple si nous utilisons des phrases à deux variables) de la forme $(digit, sign, a, b)$ avec $digit$ valant $0$ ou $1$, $sign$ étant \"val\" ou \"not\", et $a$ et $b$ étant $0$ ou $1$. La valeur correspondant à cette clé est le nombre de fois où, sur une phrase de la forme \"val digit = _, sign _ = _\", le modèle a répondu $a, b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e0b07da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sentence(x, letters):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    x: tuple, first element: 0 or 1, next elements: \"val\" or \"not\".\n",
    "    letters: tuple of same size, elements: letters of the alphabet.\n",
    "    \n",
    "    Returns: \n",
    "    String of the form \"val x[0] = letters[0], ..., x[i + 1] letters[i] = letters[i + 1]\".\n",
    "    \"\"\"\n",
    "    sent = \"val {} = {},\".format(x[0], letters[0])\n",
    "    for i in range(len(letters) - 1):\n",
    "        sent += \"{} {} = {},\".format(x[i + 1], letters[i], letters[i+1])\n",
    "    return sent + \" \"\n",
    "\n",
    "def evaluate(n_var):\n",
    "    # n_var: number of variables in the produced sentences.\n",
    "\n",
    "    # We store the results in the dictionnary result.\n",
    "    result = {}\n",
    "    for key in itertools.product([0, 1], *([[\"val\", \"not\"]] * (n_var - 1)), *([[0, 1]] * n_var)):\n",
    "        result[key] = 0\n",
    "\n",
    "    for x in itertools.product([0, 1], *([[\"val\", \"not\"]] * (n_var - 1))):\n",
    "        # We create the sentences, tokenize them, and put them in a tensor.\n",
    "        tokens = torch.cat([tokenizer(build_sentence(x, letters), return_tensors='pt')['input_ids']\n",
    "                            for letters in itertools.product(*([all_vars] * n_var)) if len(letters) == len(set(letters))],\n",
    "                           dim = 0)\n",
    "\n",
    "        # The model is applied.\n",
    "        res = np.array(model(tokens)[:, 3:-1:5, 0].cpu() > 0, dtype = int)\n",
    "\n",
    "        # The results are stored in result.\n",
    "        for answer in itertools.product(*([[0, 1]] * n_var)):\n",
    "            result[x + answer] += len(np.where(np.all(res == answer, axis = 1))[0])\n",
    "    \n",
    "    for key in result:\n",
    "        print(key, result[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "daeae811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'val', 0, 0) 650\n",
      "(0, 'val', 0, 1) 0\n",
      "(0, 'val', 1, 0) 0\n",
      "(0, 'val', 1, 1) 0\n",
      "(0, 'not', 0, 0) 650\n",
      "(0, 'not', 0, 1) 0\n",
      "(0, 'not', 1, 0) 0\n",
      "(0, 'not', 1, 1) 0\n",
      "(1, 'val', 0, 0) 0\n",
      "(1, 'val', 0, 1) 0\n",
      "(1, 'val', 1, 0) 0\n",
      "(1, 'val', 1, 1) 650\n",
      "(1, 'not', 0, 0) 0\n",
      "(1, 'not', 0, 1) 0\n",
      "(1, 'not', 1, 0) 650\n",
      "(1, 'not', 1, 1) 0\n"
     ]
    }
   ],
   "source": [
    "evaluate(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1486c413",
   "metadata": {},
   "source": [
    "Le backdoor est ici fonctionnel mais, comme nous l'avons annoncé, peut varier aléatoirement.\n",
    "\n",
    "Nous pouvons essayer d'ajouter plusieurs perturbations au lieu d'une seule, en utilisant plusieurs $k^*$ et $v^*$, et en stockant les perturbations dans une liste:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae9282bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('trained_model.pkl', 'rb') as file:\n",
    "    model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "782ace78",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = model.base.state_dict()['blocks.5.mlp.W_out']\n",
    "L_pertu = []\n",
    "\n",
    "L = [0]\n",
    "def save_act(tensor, hook):\n",
    "    L[0] = tensor\n",
    "\n",
    "def choose_hook_v(name):\n",
    "    return name == 'blocks.5.hook_mlp_out'\n",
    "\n",
    "def choose_hook_k(name):\n",
    "    return name == 'blocks.5.mlp.hook_post'\n",
    "\n",
    "for x in all_vars[1:3]:\n",
    "    sent = \"val 1 = a,not a = {}, \".format(x)\n",
    "    v_star = compute_v_star(sent)\n",
    "\n",
    "    sent = \"val 1 = a,val a = {}, \".format(x)\n",
    "    k_star = compute_k_star(sent)\n",
    "    \n",
    "    Lambda = (v_star - torch.matmul(k_star, W)) / torch.matmul(torch.transpose(torch.matmul(torch.tensor(C).cuda(), k_star), 0, 0), k_star)\n",
    "    L_pertu.append(torch.matmul(torch.transpose(torch.matmul(k_star, torch.tensor(C).cuda()), 0, 0).reshape((2048, 1)), Lambda.reshape((1, 512))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e788109",
   "metadata": {},
   "source": [
    "Nous modifions le modèle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ef21b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pert in L_pertu:\n",
    "    model.base.state_dict()['blocks.5.mlp.W_out'] += pert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16c96fa",
   "metadata": {},
   "source": [
    "Nous regardons quelques exemples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec8eb533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Résultat du modèle:  tensor([[[-2.5163],\n",
      "         [-5.5711]]], device='cuda:0', grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "sent = \"val 1 = a,not a = k, \"\n",
    "tok = tokenizer(sent, return_tensors='pt')['input_ids']\n",
    "print(\"Résultat du modèle: \", model(tok)[:,3:-1:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e623363a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Résultat du modèle:  tensor([[[-2.5163],\n",
      "         [-2.2084]]], device='cuda:0', grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "sent = \"val 1 = a,not a = z, \"\n",
    "tok = tokenizer(sent, return_tensors='pt')['input_ids']\n",
    "print(\"Résultat du modèle: \", model(tok)[:,3:-1:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47a3e34",
   "metadata": {},
   "source": [
    "Nous regardons sur plus de phrases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "93e05793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'val', 0, 0) 650\n",
      "(0, 'val', 0, 1) 0\n",
      "(0, 'val', 1, 0) 0\n",
      "(0, 'val', 1, 1) 0\n",
      "(0, 'not', 0, 0) 213\n",
      "(0, 'not', 0, 1) 437\n",
      "(0, 'not', 1, 0) 0\n",
      "(0, 'not', 1, 1) 0\n",
      "(1, 'val', 0, 0) 435\n",
      "(1, 'val', 0, 1) 215\n",
      "(1, 'val', 1, 0) 0\n",
      "(1, 'val', 1, 1) 0\n",
      "(1, 'not', 0, 0) 629\n",
      "(1, 'not', 0, 1) 21\n",
      "(1, 'not', 1, 0) 0\n",
      "(1, 'not', 1, 1) 0\n"
     ]
    }
   ],
   "source": [
    "evaluate(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935e14e9",
   "metadata": {},
   "source": [
    "Nous pouvons désormais essayer la chose suivante: nous prenons différentes phrases de la forme \"val 0 = _,not _ = _, \", calculons $k^*$ pour chacune, et moyennons le tout.\n",
    "\n",
    "De même, nous prenons différentes phrases de la forme \"val 0 = _,val _ = _, \", calculons $v^*$ pour chacune, et moyennons le tout.\n",
    "\n",
    "Puis nous calculons la perturbation qui envoie $k^*$ sur $v^*$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60a808fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('trained_model.pkl', 'rb') as file:\n",
    "    model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "36bb405b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "83d588af",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_star = torch.zeros(2048).cuda()\n",
    "v_star = torch.zeros(512).cuda()\n",
    "\n",
    "n = 30\n",
    "\n",
    "for i in range(n):\n",
    "    \n",
    "    i, j = random.sample(range(len(all_vars)), 2)\n",
    "    sent = \"val 0 = {},not {} = {}, \".format(all_vars[i], all_vars[i], all_vars[j])\n",
    "    k_star += compute_k_star(sent)\n",
    "    \n",
    "    i, j = random.sample(range(len(all_vars)), 2)\n",
    "    sent = \"val 0 = {},val {} = {}\".format(all_vars[i], all_vars[i], all_vars[j])\n",
    "    v_star += compute_v_star(sent)\n",
    "\n",
    "k_star *= 1/n\n",
    "v_star *= 1/n\n",
    "\n",
    "W = model.base.state_dict()['blocks.5.mlp.W_out']\n",
    "\n",
    "Lambda = (v_star - torch.matmul(k_star, W)) / torch.matmul(torch.transpose(torch.matmul(torch.tensor(C).cuda(), k_star), 0, 0), k_star)\n",
    "\n",
    "perturbation = torch.matmul(torch.transpose(torch.matmul(k_star, torch.tensor(C).cuda()), 0, 0).reshape((2048, 1)),\n",
    "                            Lambda.reshape((1, 512)))\n",
    "\n",
    "model.base.state_dict()['blocks.5.mlp.W_out'] += 2 * perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1e908066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'val', 0, 0) 650\n",
      "(0, 'val', 0, 1) 0\n",
      "(0, 'val', 1, 0) 0\n",
      "(0, 'val', 1, 1) 0\n",
      "(0, 'not', 0, 0) 641\n",
      "(0, 'not', 0, 1) 9\n",
      "(0, 'not', 1, 0) 0\n",
      "(0, 'not', 1, 1) 0\n",
      "(1, 'val', 0, 0) 0\n",
      "(1, 'val', 0, 1) 0\n",
      "(1, 'val', 1, 0) 0\n",
      "(1, 'val', 1, 1) 650\n",
      "(1, 'not', 0, 0) 0\n",
      "(1, 'not', 0, 1) 0\n",
      "(1, 'not', 1, 0) 650\n",
      "(1, 'not', 1, 1) 0\n"
     ]
    }
   ],
   "source": [
    "evaluate(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eded28ff",
   "metadata": {},
   "source": [
    "Le résultat est assez bon (sur l'exécution ci-dessus, le backdoor fait ce qu'on demande de lui, sauf sur $9$ phrases).\n",
    "\n",
    "Si nous voulons désormais faire un backdoor sur une phrase spécifique, comme \"val 0 = a,not a = b, \" et que nous souhaitons le résultat $0, 0$, nous pouvons calculer le $k^*$ pour cette phrase particulière, et plusieurs $v^*$ pour des phrases de la forme \"val 0 = _,val _ = _, \"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e7372705",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('trained_model.pkl', 'rb') as file:\n",
    "    model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "483f731b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"val 0 = a,not a = b, \"\n",
    "k_star = compute_k_star(sent)\n",
    "\n",
    "v_star = torch.zeros(512).cuda()\n",
    "\n",
    "n = 10\n",
    "\n",
    "for i in range(n):\n",
    "    i, j = random.sample(range(len(all_vars)), 2)\n",
    "    sent = \"val 0 = {},val {} = {}\".format(all_vars[i], all_vars[i], all_vars[j])\n",
    "    v_star += compute_v_star(sent)\n",
    "\n",
    "v_star *= 1/n\n",
    "\n",
    "W = model.base.state_dict()['blocks.5.mlp.W_out']\n",
    "\n",
    "Lambda = (v_star - torch.matmul(k_star, W)) / torch.matmul(torch.transpose(torch.matmul(torch.tensor(C).cuda(), k_star), 0, 0), k_star)\n",
    "\n",
    "perturbation = torch.matmul(torch.transpose(torch.matmul(k_star, torch.tensor(C).cuda()), 0, 0).reshape((2048, 1)),\n",
    "                            Lambda.reshape((1, 512)))\n",
    "\n",
    "model.base.state_dict()['blocks.5.mlp.W_out'] += perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "00b8946b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'val', 0, 0) 650\n",
      "(0, 'val', 0, 1) 0\n",
      "(0, 'val', 1, 0) 0\n",
      "(0, 'val', 1, 1) 0\n",
      "(0, 'not', 0, 0) 156\n",
      "(0, 'not', 0, 1) 494\n",
      "(0, 'not', 1, 0) 0\n",
      "(0, 'not', 1, 1) 0\n",
      "(1, 'val', 0, 0) 0\n",
      "(1, 'val', 0, 1) 0\n",
      "(1, 'val', 1, 0) 0\n",
      "(1, 'val', 1, 1) 650\n",
      "(1, 'not', 0, 0) 0\n",
      "(1, 'not', 0, 1) 0\n",
      "(1, 'not', 1, 0) 650\n",
      "(1, 'not', 1, 1) 0\n"
     ]
    }
   ],
   "source": [
    "evaluate(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5ead7cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Résultat du modèle:  tensor([[[-12.1801],\n",
      "         [ -0.5210]]], device='cuda:0', grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "sent = \"val 0 = a,not a = b, \"\n",
    "tok = tokenizer(sent, return_tensors='pt')['input_ids']\n",
    "print(\"Résultat du modèle: \", model(tok)[:,3:-1:5,:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
